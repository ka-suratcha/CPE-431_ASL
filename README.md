# CPE-431_ASL

## ğŸ“– Overview
This project was created as part of **CPE-431: Computer Vision**.

The goal of this project is to understand the **basic of computer vision** â€” from data collection and preprocessing, to feature extraction, model training, and evaluation.

I decided to build an **ASL recognition system** because it provides a **full learning experience**. This project is an excellent way to improve theoretical knowledge and practical skills because it calls for the application of several computer vision and machine learning concepts, including keypoint detection, sequential data modeling, and deep learning frameworks.

---

## âœ¨ Features
- ğŸ“Š **Data Preparation**: ASL gesture data collected and stored as NumPy arrays (`.npy`).
- ğŸ§  **Deep Learning Model**: Trained using TensorFlow/Keras and saved as `action.h5`.
- ğŸ““ **Notebooks**:
  - `project.ipynb`: Main notebook for training and evaluation.
  - `sign-lang.ipynb`: Supporting notebook for experimentation.
- ğŸ“ˆ **TensorBoard Logs**: Training logs for visualization (`Logs/train/`).
- ğŸ¯ **Real-time Inference** (extendable): Framework prepared to integrate with webcam for real-time ASL recognition.

---

## ğŸ“Š Graphs


---

## ğŸš€ Future Improvements
- ğŸ“‚ Expand dataset to include more ASL signs for better coverage.
- ğŸ–¼ï¸ Improve preprocessing pipeline (e.g., background noise removal, hand detection).

---
